{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Dataset Description Extraction\n",
    "\n",
    "This notebook will assist with web scraping the descriptions for the various datasets located on https://www.imdb.com/interfaces/. You must be connected to the Guest Nationwide network or be off of Nationwide network to execute the notebook.\n",
    "\n",
    "The notebook will parse through the html content and isolate on the class tag called _blurb_. Once the tag is found, we'll parse through html to find the paragraph tag `<p>`. This tag encloses the dataset name. Then performing another prasing exercise, we'll look for the line item tag `<li>`. That tag contains the individual field descriptions.\n",
    "    \n",
    "After all the parsing is complete, a CSV file will be generated for each dataset containing the field descriptions for that file. The file can be imported into other notebooks for exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# specify the url\n",
    "url = 'https://www.imdb.com/interfaces/'\n",
    "\n",
    "# fetch url\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse the html using beautiful soup and store in variable `soup`\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the class blurb section of the page\n",
    "divs = soup.find_all('p', {'class': 'blurb'})\n",
    "\n",
    "# validate that the class was found.\n",
    "len(divs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture the results\n",
    "div = divs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse through the results and look for the bold attribute - the dataset is encased in bold tags: <b>title</b>\n",
    "# store the data set names in a list for future use\n",
    "\n",
    "dataset = []\n",
    "for section in div.find_all('b'):\n",
    "    ds = section.find_all(text=True)[0]\n",
    "    dataset.append(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete 'Data Location' from the dictionary\n",
    "del dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete 'IMDb Dataset Details' from the dictionary - the list gets reorder\n",
    "del dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse through the results and look for the list item attribute - the field descriptions are encased in li tags: <li>title</li>\n",
    "# store the field descriptions in a list for future use\n",
    "description = []\n",
    "for section in div.find_all('li'):\n",
    "    des = section.find_all(text=True)[0]\n",
    "    description.append(des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "fldDescription = []\n",
    "\n",
    "# interate through the field descriptions and break apart the elements\n",
    "# use regular expression library to locate and replace any en-dash characters\n",
    "\n",
    "for i in description:\n",
    "    txt = re.sub(u\"\\u2013\", \"-\", i) # convert all en-dash to utf-8 dash\n",
    "    txt = txt.replace('‘', \"'\") # convert grave accent to utf-8 single quote\n",
    "    txt = txt.replace('’', \"'\") # convert acute accent to utf-8 single quote\n",
    "    txt = txt.split('-', 1) # split txt on the first occurence of a dash\n",
    "    fldDesc = txt[1].strip()\n",
    "    txt = txt[0].split(' ', 1) # split txt on the first occurence of a space\n",
    "    fldName = txt[0].strip()\n",
    "    fldType = txt[1].strip()\n",
    "    fldType = fldType.replace('(', '').replace(')', '') # remove the parenthesis around type value\n",
    "    fldDescription.append([fldName, fldType, fldDesc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a header row\n",
    "HEADER = ['fldName', 'fldType', 'fldDesc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add header and copy the first 8 indexes from field description for title.akas\n",
    "file1 = []\n",
    "file1.append(HEADER)\n",
    "for i in fldDescription[0:8]:\n",
    "    file1.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add header and copy the next 9 indexes from field description for title.basics\n",
    "file2 = []\n",
    "file2.append(HEADER)\n",
    "for i in fldDescription[8:17]:\n",
    "    file2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add header and copy the next 3 indexes from field description for title.crew\n",
    "file3 = []\n",
    "file3.append(HEADER)\n",
    "for i in fldDescription[17:20]:\n",
    "    file3.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add header and copy the next 4 indexes from field description for title.episode\n",
    "file4 = []\n",
    "file4.append(HEADER)\n",
    "for i in fldDescription[20:24]:\n",
    "    file4.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add header and  copy the next 6 indexes from field description for title.principals\n",
    "file5 = []\n",
    "file5.append(HEADER)\n",
    "for i in fldDescription[24:30]:\n",
    "    file5.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add header and  copy the next 3 indexes from field description for title.ratings\n",
    "file6 = []\n",
    "file6.append(HEADER)\n",
    "for i in fldDescription[30:33]:\n",
    "    file6.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add header and copy the remaining indexes from field description for names.basics\n",
    "file7 = []\n",
    "file7.append(HEADER)\n",
    "for i in fldDescription[33:]:\n",
    "    file7.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def writeFile(fileName, listin):\n",
    "\n",
    "    # format the output file name\n",
    "    fileName = fileName.split('.')\n",
    "    fileName = fileName[0] + '_' + fileName[1]\n",
    "    \n",
    "    # write field description list to a comma delimited seperated file\n",
    "    with open('./data/' + fileName + '_flddesc.csv', \"w\", newline='') as f:\n",
    "        writer = csv.writer(f,delimiter='^',)\n",
    "        writer.writerows(listin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write field descriptions for title.akas to csv file\n",
    "writeFile(dataset[0], file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write field descriptions for title.basics to csv file\n",
    "writeFile(dataset[1], file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write field descriptions for title.crew to csv file\n",
    "writeFile(dataset[2], file3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write field descriptions for title.episode to csv file\n",
    "writeFile(dataset[3], file4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write field descriptions for title.principles to csv file\n",
    "writeFile(dataset[4], file5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write field descriptions for title.ratings to csv file\n",
    "writeFile(dataset[5], file6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write field descriptions for name basics to csv file\n",
    "writeFile(dataset[6], file7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
